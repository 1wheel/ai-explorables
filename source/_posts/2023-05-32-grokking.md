---
template: post_v2.html
title: Memorization and Generalization
title: Do Machine Learning Models Memorize or Generalize?
socialsummary: 
shareimg: TKTK
authors: Adam Pearce, Asma Ghandeharioun, Nada Hussein, Nithum Thain, Martin Wattenberg, Lucas Dixon
date: August 2023
permalink: /grokking/
---

Do machine learning models only parrot back their training data or do they develop useful internal models of the world <a class='citestart' key='Parrots Othello'></a>? Large language models can certainly seem like they have learned rich mental models of the world, but they might just be just regurgitating memorized bits of the enormous amount of data they've been trained on.

In 2021, researchers made a striking discovery while training a series of tiny models on toy tasks. They found a set of models that suddenly flipped from memorizing their training data to correctly generalizing on unseen inputs after training for much longer. This phenomenon – where generalization happens seemingly abruptly and long after overfitting the training data – is called *grokking* and has sparked a flurry of interest <a class='citestart' key='Grokking Omnigrok Universality Zhong23'></a>. 

<div class='sticky-container'>
<div class='mod-top-accuracy row sticky'></div>

Do large models also suddenly generalize after they're trained longer? Can we detect if they're stuck memorizing? In this article we'll examine the training dynamics of a tiny model and reverse engineer its generalizing solution <a class='citestart' key='MechInterp ProgressMeasures'></a>. While it isn't clear how to reverse engineer the largest models to definitely answer our questions about them, starting small will build up our intuition and understanding.

### Grokking Modular Addition 

We'll take a close look at modular addition, essentially the fruit fly of grokking.<a class='footstart' key='modular'></a> In particular, the above line chart comes from a model trained to predict `$a + b \bmod 67$`.<a class='footstart' key='67' ></a> We randomly divide all the `$a, b$` pairs into test and training datasets. Over thousands of training steps, the training data is used to adjust the model into outputting correct answers, while the test data is only used to check if the model has learned a general solution.

The model's architecture is similarly simple: `$\text{ReLU}\left(\mathbf{a}_{\text{one-hot}} \mathbf{W}_{\text{input}} + \mathbf{b}_{\text{one-hot}} \mathbf{W}_{\text{input}}\right) \mathbf{W}_{\text{output}}
$` — a one-layer MLP with 24 neurons.<a class='footstart' key='playground'></a> All the weights of the model are shown in the heatmap below; you can see how they change during training by mousing over the line chart. 

<div class='sticky-container'>
<div class='mod-top-weights row x-sticky x-sticky-lower'></div>

The model makes a prediction by selecting the two columns of `$\mathbf{W}_{\text{input}}$` corresponding to inputs `$a$` and `$b$` then adds them together to create a vector of 24 separate numbers. Next it sets all the negative numbers in the vector to 0 and finally outputs the column of `$\mathbf{W}_{\text{output}}$` that's closest to the updated vector.

The weights of the model are initially quite noisy but start to exhibit periodic patterns as accuracy on the test data increases and the model <animate data-animate='top-switches'>switches</animate> to generalizing. By the end of training, each neuron — each row of the heatmap — cycles through high and low values several times as the input number increases from 0 to 66.

This is easier to see if we group the neurons by how often they cycle at the end of training and chart each of them as a separate line.  

</div>
</div>
<div class='mod-top-waves row'></div>

The periodic patterns suggest the model is learning some sort of general, mathematical solution. But *why* does the model move away from the memorizing solution? And *what* is the generalizing solution?   

### Generalizing With 1s and 0s 

Figuring out both of these questions simultaneously is hard. Let's make  an even simpler task, one where we know what the generalizing solution should look like and try to understand why the model eventually learns it.

We'll take random sequences of thirty 1s and 0s and train our model to predict if there is an odd number of 1s in the first three digits. e.g. <digits>000110010110001010111001001011</digits> is <digits>0</digits> while <digits>010110010110001010111001001011</digits> is <digits>1</digits> — basically a slightly trickier XOR with some distraction noise. A generalizing model should only use the first three digits of the sequence; if the model is memorizing the training data, it will also use the subsequent distracting digits <a class='citestart' key='ProgressParity TwoCircuits'></a>.   

Our model is again a one-layer MLP, trained on a fixed batch of 1,200 sequences.<a class='footstart' key='sp-model'></a> Initially only training accuracy increases — the model is memorizing the training data. As with modular arithmetic, test accuracy is essentially random and then sharply increases as the model learns a general solution.

<div class='parity-accuracy row'></div>

While <animate data-animate='sp-mem'>memorizing</animate> , the model looks dense and noisy with lots of high magnitude weights (shown as dark red and blue squares) spread across the chart – the model is using all of the inputs to make a prediction. As the model <animate data-animate='sp-gen'>generalizes</animate> and gets perfect test accuracy, we see all the weights connected to the distracting digits gray out with very low values and the model focusing on the first three digits — mirroring the generalized structure we expected!

<div class='parity-weights row'></div>

This happens because we're pushing our model to do two things during training — output a high probability for the correct label (called minimizing *loss*<a class='footstart' key='loss'></a>) and have weights with low magnitudes (known as *weight decay*). Before the model generalizes, train loss actually slightly increases — it's trading off between outputting the correct label and having low weights. 

<div class='parity-loss row'></div>

The sharp drop in test loss makes it appear like the model makes a sudden shift to generalization. If we look at the weights of the model over training though, they're mostly smoothly interpolating between the two solutions. The rapid generalization occurs when the last weights connected to the distracting digits are pruned by weight decay.

<div class='parity-weights-trajectory row'></div>
    
### When Does Grokking Happen?

It's important to note that grokking is a contingent phenomenon — it goes away if model size, weight decay, data size and other hyper parameters aren't just right. With too little weight decay, the model can overfit<a class='footstart' key='overfit'></a> to the training data. Adding more weight decay pushes the model to generalize after memorizing. Increasing weight decay even more causes test and train loss to fall together; the model goes straight to generalizing. And with too much weight decay the model will fail to learn anything. 

Below, we've trained over a thousand models on the 1s and 0s task with different hyperparameters. Training is noisy so nine models have been trained for each set of hyperparameters. 

<div class='sparse-parity-sweep'></div>

<br>
<br>
<br>


We can induce memorization and generalization on this somewhat contrived 1s and 0s task — why does it happen with the modular addition? Let's first understand a little bit more about how a one-layer MLP can solve modular addition by constructing a generalizing solution that's interpretable. 
### Modular Addition With Five Neurons

Recall that our modular arithmetic problem `$a + b \bmod 67$` is naturally periodic, with answers wrapping around if the sum ever passes 67. Mathematically, this can be mirrored by thinking of the sum as wrapping `$a$` and `$b$` around a circle. We also saw periodic patterns in the learned weights, indicating the model is . 

Let’s make a simpler model with a head start on the problem. We'll construct an embedding matrix that places `$a$` and `$b$` on a circle using sine and cosine functions.

`$$
\mathbf{W}_{\text{embed}} =  \left[ \cos\frac{2\pi i}{67}, \; \sin\frac{2\pi i}{67} \right]
$$`
<div class='row'><div class='embed'></div></div>

<br>
Then we train `$\mathbf{W}_{\text{in-proj}}$` and `$\mathbf{W}_{\text{out-proj}}$` in this one-layer MLP:

<div style='height: 1px;margin-top:-10px'></div>

<script type="math/tex">
$$
\begin{aligned}
\text{activations} & = \text{ReLU}\left(\mathbf{a}_{\text{one-hot}} \mathbf{W}_{\text{embed}} \mathbf{W}_{\text{in-proj}} + \mathbf{b}_{\text{one-hot}} \mathbf{W}_{\text{embed}} \mathbf{W}_{\text{in-proj}}\right) \\

\text{logits} & = \text{activations} \mathbf{W}_{\text{out-proj}} \mathbf{W}_{\text{embed}}^{\top}
\end{aligned}
$$
</script>

<div style='height: 1px;margin-top:-10px'></div>

The model quickly converges to a solution with perfect accuracy — this works with just five neurons. 

<div class='row'>
  <div class='hiddenW'></div>
  <div class='outW'></div>
</div>

Intriguingly, the trained parameters of `$\mathbf{W}_{\text{in-proj}}$` are evenly distributed around a circle; we can reorder the neurons then scale and rotate to cleanly place them on the unit circle.

<div class='row'>
  <div class='circle-hiddenWT'></div>
  <div class='circle-outW'></div>
</div>

When this transformation is applied, we can clearly see `$\mathbf{W}_{\text{out-proj}}$` rotating around the circle twice as fast as `$\mathbf{W}_{\text{in-proj}}$`. 

The details of how this solution works aren't essential —  checkout [Appendix A](#appendix-a-why-the-circular-construction-works) to see how the doubled rotation let's the model map inputs like `$1 + 0 \bmod 67$`  and `$2 + 66 \bmod 67$` to the same place — but we have constructed a 20 parameter model that solves modular addition. Can we find the same algorithm in the 3,216 parameter model we started with? And why does the larger model switch to the generalizing solution after memorizing? 
### Full of Stars

Here's the `$a + b \bmod 67$` model that we started with again — it's trained from scratch with no built in periodicity. 

<div class='sticky-container'>
<div class='mod-bot-accuracy sticky'></div>

<div class='mod-bot-waves row'></div> 

We can take the discrete Fourier transform (DFT)<a class='footstart' key='dft'></a> of the weights to factor out the learned periodic patterns and leave us with the equivalent of `$\mathbf{W}_{\text{in-proj}}$` and `$\mathbf{W}_{\text{out-proj}}$` from the constructed solution. Just like in the 1s and 0s task, weight decay encourages this representation to become much sparser as the model <animate data-animate='bot-gen'>generalizes</animate> (the neurons have been sorted by frequency and phase here). 

<div class='mod-bot-dft row'></div>

If we pick out the frequencies that the model settles on and plotting the `$\cos$` and `$\sin$` components of the DFT of every neuron, we see the same star shapes from the constructed solution appear.

<div class='mod-bot-freqs-hidden row'></div>

<div class='mod-bot-freqs-out row'></div>

**This trained model is using the same algorithm as our constructed solution!** If we look at the logits generated by each frequency for a given input, we can see them calculating  `$ \cos\frac{2\pi (a +  b) freq}{67}$` separately.

Notice what happens to frequency 7 when test loss <animate data-animate='bot-improve'>improves</animate> after plateauing at 45k steps — its neurons start to snap into a star shape and their logits more closely approximate a wave.        

<div class='row mod-bot-sliders'></div>

<div class='row mod-bot-logits'></div>

To lower loss without higher weights (which would be punished by weight decay) the model uses several frequencies, taking advantage of constructive interference. There's nothing magical about the frequencies 4, 5, 7 and 26 — click through other training running runs below to see the same algorithm get learned in different ways.  

</div>
<div class='row mod-bot-seeds'></div>



### Open Questions

We've glossed over some details while writing this post. Here's some things we're still curious about. 
#### Which Model Constraints Work Best?

Directly training the model visualized above — `$\text{ReLU} \left(a_{\text{one-hot}}\textbf{W}_{\text{input}} + b_{\text{one-hot}}\textbf{W}_{\text{input}} \right) \textbf{W}_{\text{output}}$` — does not actually result in generalization on modular arithmetic, even with weight decay added. At least one of the matrices has to be factored:  

`$$
\textbf{W}_{\text{input}} = \mathbf{W}_{\text{embed}} \mathbf{W}_{\text{in-proj}}
$$`

`$$
\textbf{W}_{\text{output}} = \textbf{W}_{\text{out-proj}} \textbf{W}_{\text{embed}}^{\top}
$$` 

We observed that the generalizing solution is sparse after taking the discrete Fourier transformation on the collapsed matrices, but has high norms prior to Fourier transformation. This suggests that weight decay on `$\textbf{W}_\text{output}$` and `$\textbf{W}_{\text{input}}$` doesn't provide the right inductive bias for the task. 

Broadly speaking, weight decay does steer a wide variety of models away from memorizing their training data <a class='citestart' key='double-demystified DoubleDescent'></a>. Other techniques that avoid overfitting include dropout, smaller models and even numerically unstable optimization algorithms <a class='citestart' key='Slingshot'></a>. These approaches interact in complex, non-linear ways, making it difficult to predict a priori what adjustments will ultimately induce generalization. Get a glimpse of the interactions between them [here](https://pair-code.github.io/tiny-transformers/mlp-modular/02-sweep-architecture)!

#### Why Is Memorization Easier Than Generalization? 

One theory: there can be many more ways to memorize a training set than there are generalizing solutions. So statistically, memorization should be more likely to happen first, especially if we have no or little regularization. Regularization techniques, like weight decay, prioritize certain solutions over the others, for example, preferring “sparse” solutions over “dense” ones.

Recent work suggests that generalization is associated with well-structured representations <a class='citestart' key='EffectiveTheory'></a>. However it's not a necessary condition; some MLP variations without symmetric inputs learn less "circular" representations when solving modular addition <a class='citestart' key='Zhong23'></a>. We also observed that well-structured representations are not a sufficient condition for generalization. This small model (trained with no weight decay) starts generalizing, then switches to memorizing with periodic embeddings. 

<div class='open-q-mem-0-accuracy row'></div>
<div class='open-q-mem-0-weights  row'></div>

It's even possible to find hyperparameters where models start generalizing, then switch to memorizing, then switch back to generalizing! 

<div class='open-q-mem-1-accuracy row'></div>
<div class='open-q-mem-1-weights  row'></div>

"Rules" for larger models are hard to find since there are so many strange train trajectories.

#### What About Larger Models?

Does grokking happen in larger models trained on real world tasks? Earlier observations reported the grokking phenomenon in algorithmic tasks in small transformers and MLPs <a class='citestart' key='Grokking ProgressMeasures Zhong23'></a>. Grokking has subsequently been found in more complex tasks involving images, text, and tabular data within certain rangers of hyperparameters <a class='citestart' key='Goldilocks Omnigrok'></a>. It's possible that the largest models, which are able to do many types of tasks, may be grokking many things at different speeds during training <a class='citestart' key='quantization'></a>.

Understanding the solution to modular addition wasn't trivial — do we have any hope of understanding larger models? One route forward: 1) train simpler models with more inductive biases/fewer moving parts, 2) use them to explain the remaining inscrutable parts of how a model works, 3) repeat as needed. We believe this could be a fruitful approach to better understanding larger models, and complementary to efforts that aim to use larger models to explain smaller ones and other efforts to disentangle internal representations <a class='citestart' key='explain multiple-choice TMOS'></a>.  

There have also been promising results in predicting grokking before it happens. Though some require knowledge of the generalizing solution <a class='citestart' key='ProgressMeasures'></a> or the overall data domain <a class='citestart' key='StructuralGrokking'></a>, some rely solely on the analysis of the training loss <a class='citestart' key='PredictingGrokking'></a> and might also apply to larger models — hopefully we'll be able to build tools and techniques that can tell us when model is parroting memorized information and when it is using richer models.

### Credits 

Thanks to Ardavan Saeedi, Crystal Qian, Emily Reif, Fernanda Viégas, Kathy Meier-Hellstern, Minsuk Chang and Ryan Mullins for their help with this piece. 


### Appendix A: How the Circular Construction Works

We can almost calculate `$a + b  \bmod 67$` using two circular embeddings and a completely linear model.

<div class='sticky-container'>
<div class='slider-container row sticky appendix'></div>
<div class='circle-vis row'></div>

<p>It works! But we're cheating a bit, do you see how **unembed** loops around the circle twice? We need to output a single prediction for "<v></v>" — not separate predictions for "<v></v>" and "<v2></v2>". Directly adding the two predictions for a number together won't work since they're on opposite sides of the circles and will cancel each other out. 

Instead, let's incorporate a few non-linearity to fix the repeated outputs. 

<div class='proj-vis row'></div>

We've essentially curled the circle around in on itself and the model outputs a single prediction for "<v></v>".   

Formally, this is the constructed model: 

<script type="math/tex">
$$
\begin{aligned}
\text{activations} & = \text{ReLU}\left(\mathbf{a}_{\text{one-hot}} \mathbf{W}_{\text{embed}} \mathbf{W}_{\text{in-proj}} + \mathbf{b}_{\text{one-hot}} \mathbf{W}_{\text{embed}} \mathbf{W}_{\text{in-proj}}\right) \\

\text{logits} & = \text{activations} \mathbf{W}_{\text{out-proj}} \mathbf{W}_{\text{embed}}^{\top}
\end{aligned}
$$
</script>

With `$N$` evenly spaced neurons/directions:  

<script type="math/tex">
$$
\mathbf{W}_{\text{in-proj}}^T = 
\begin{pmatrix}
    \dots & \ldots \\
    \cos(i\frac{2\pi}{N}) & \sin(i \frac{2\pi}{N}) \\
    \dots & \dots \\
\end{pmatrix}, \quad

\mathbf{W}_{\text{out-proj}} = 
\begin{pmatrix}
    \dots & \dots \\
    \cos(2i\frac{2\pi}{N}) & \sin(2i\frac{2\pi}{N}) \\
   \dots & \dots \\
\end{pmatrix}.
$$
</script>

<br>

<p>Interestingly the curled circle has a few wrinkles: the construction doesn't give an exact answer! 

</div>

<div class='debug-vis row'></div>
<div class='appendix num-inputs row'>
  <span>Neurons <input type="number" class='n_neurons' min="3" max="10" value="5"></span>
  <span>Modulus <input type="number" class='modulus' min="12" max="500" value="67"></span>
</div>
<br>

<p>If we drop the discrete embeddings to focus on how the model doubles the angle of input's midpoint, we can see the whole things is built atop a strange trig approximation:

`$$
2 \theta \approx 
  \tan^{-1}
  \left(
  \frac{
    \sum_{i=0}^{N-1} 
    \text{ReLU}
    \left[
      \cos
      \left(
        \theta - i\frac{2\pi}{N}
      \right)
    \right] 
    \cdot 
    \sin
    \left(
      2i\frac{2\pi}{N}
    \right)
}
    {
\sum_{i=0}^{N-1} 
    \text{ReLU}
    \left[
      \cos
      \left(
        \theta - i\frac{2\pi}{N}
      \right)
    \right] 
    \cdot 
    \cos
    \left(
      2i\frac{2\pi}{N}
    \right)
  }
 \right)
$$`

<br>

<div class='debug-reuleaux row'></div>



### Footnotes

<a class='footend' key='modular'></a> In modular addition, we have two input numbers, `$a$` and `$b$`, and a modulus `$m$`. We want to find the remainder of `$a + b$` when divided by `$m$`. 
<span class='fn-break'></span>
This type of addition is often called clock-face addition, because when adding two times, we often report the result modulo 12 (i.e. 5 hours after 8 o’clock is 1 o’clock).
<span class='fn-break'></span>
Modular addition sounds simple and it is — we can easily train 1,000s of models and treat them like fruit flies in neuroscience: small enough such that it is feasible to extract their [connectome](https://www.science.org/doi/abs/10.1126/science.add9330) synapse-by-synapse, yet providing new interesting insights about the system more broadly. We can get a good understanding of what these small models do by visualizing all their internals.


<a class='footend' key='67'></a>67 isn't a magic number – not so small that the task is trivial, but also not so large that the visualizations are overwhelming. Also it is prime, which simplifies the [group structure](https://en.wikipedia.org/wiki/Group_theory).  


<a class='footend' key='playground'></a>[playground.tensorflow.org](http://playground.tensorflow.org) is a great place to start if you're not familiar with [MLPs](https://en.wikipedia.org/wiki/Multilayer_perceptron).
<span class='fn-break'></span>
Also: the model is trained with cross-entropy loss, AdamW and full batches. The [regularization section](#regularization) and training colab have additional details.  


<a class='footend' key='sp-model'></a> With a small twist — we're only outputting 1 or 0, so `$\mathbf{W}_{\text{output}}$` can be a single column. In the modular addition task we needed a column for every output number.
<span class='fn-break'></span>
The last column of `$\mathbf{W}_{\text{input}}$` is also fixed to 1 to provide a bias term.


<a class='footend' key='loss'></a> So far we've been charting [accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy), the percentage of sequences where the correct label is the most likely. Training typically instead optimizes a differentiable objective function. All the models in this post use [cross entropy loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html) which heavily penalizes incorrect predictions with high probabilities**. 
**<span class='fn-break'></span>
Note that while some formulations of loss include a weight decay or regularization term, the loss plots here depict the cross entropy component alone.


<a class='footend' key='overfit'></a> Overfitting describes a state where machine learning models perform with high accuracy on the training data but low accuracy on the test data — what we see with our memorizing models. In general, simpler models are less prone to overfitting as, due to their simplicity, decision rules are coarser and are required to make more generalizations. Of course, if a model is too simple for a task, it may not be able to learn good decision rules that capture the nuances of the task. Researchers force models to be simpler through a variety of techniques, including having models with fewer parameters or encouraging the parameters that the model does have to be small in size.  


<a class='footend' key='circles'></a>The first row shows each neuron's activations. The activation patterns of different neurons look similar, except for the incremental shift toward the up-right corner. This follows the circle structure we observed in `$\textbf{W}_{\text{in-proj}}$`. Notice the big chunks of gray areas. They indicate where a neuron is "off," meaning its activation is zeroed out due to the ReLU function. For any pair of `$a$` and `$b$`, there are always either two or three neurons simultaneously "on." This intriguing behavior follows the circle structure in `$\textbf{W}_{\text{in-proj}}$`, along with the ReLU function.
<span class='fn-break'></span>
The second row shows how much each neuron contributes to the final logit value, which is determined by scaling the corresponding row of `$\textbf{W}_{\text{out-proj}} \textbf{W}^{\top}_{\text{embed}}$` by the neuron’s activation. The star structure in `$\textbf{W}_{\text{out-proj}}$` that we previously observed results in the sinusoidal shape with incremental phase shifts in these logit plots. For each input pair `$a$` and `$b$`, only the neurons that are "on" contribute to the final logit value.
<span class='fn-break'></span>
Finally, the third row shows the total logit sum and highlights the output with the highest logit value. Voila! That is `$a+b \bmod 67$`.


<a class='footend' key='dft'></a> The [Discrete Fourier Transform](https://www.youtube.com/watch?v=g8RkArhtCc4&t=1285s) helps analyze the periodic nature of a sequence of values (in this case the weights for a particular neuron) by breaking it down into sin and cos functions. The more periodic a function is, the easier it is to represent with sin and cosines, and the sparser the output of the DFT.

### References

<a class='citeend' key='Parrots'></a>[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?🦜](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922?uuid=f2qngt2LcFCbgtaZ2024) Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). *In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency* (pp. 610-623).

<a class='citeend' key='Othello'></a> [Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task](https://openreview.net/pdf?id=DeG07_TcZvT) Li, K., Hopkins, A. K., Bau, D., Viégas, F., Pfister, H., & Wattenberg, M. (2022, September). *In The Eleventh International Conference on Learning Representations*.

<a class='citeend' key='Grokking'></a> [Grokking: Generalization Beyond Overfitting On Small Algorithmic Datasets](https://arxiv.org/pdf/2201.02177.pdf)
Power, A., Burda, Y., Edwards, H., Babuschkin, I., & Misra, V. (2022). arXiv preprint arXiv:2201.02177.

<a class='citeend' key='Omnigrok'></a> [Omnigrok: Grokking Beyond Algorithmic Data](https://arxiv.org/pdf/2210.01117.pdf)
Liu, Z., Michaud, E. J., & Tegmark, M. (2022, September). In The Eleventh International Conference on Learning Representations.

<a class='citeend' key='Universality'></a> [A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations](https://arxiv.org/abs/2302.03025)
Chughtai, B., Chan, L., Nanda, N.  (2023). International Conference on Machine Learning.

<a class='citeend' key='Zhong23'></a>[The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks](https://arxiv.org/pdf/2306.17844.pdf)
Zhong, Z., Liu, Z., Tegmark, M., & Andreas, J. (2023). arXiv preprint arXiv:2306.17844.

<a class='citeend' key='MechInterp'></a> [Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)
Olah, C., 2022. Transformer Circuits Thread. 

<a class='citeend' key='ProgressMeasures'></a> [Progress Measures for Grokking via Mechanistic Interpretability](https://openreview.net/pdf?id=9XFSbDPmdW)
Nanda, N., Chan, L., Lieberum, T., Smith, J., & Steinhardt, J. (2022, September). In The Eleventh International Conference on Learning Representations.

<a class='citeend' key='ProgressParity'></a> [Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit](https://arxiv.org/abs/2207.08799)
Boaz Barak, Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran Malach, Cyril Zhang. (2022) Advances in Neural Information Processing Systems, 35, 21750-21764.

<a class='citeend' key='TwoCircuits'></a> [A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks](https://arxiv.org/abs/2303.11873)
William Merrill, Nikolaos Tsilivis, Aman Shukla. (2023). arXiv preprint arXiv:2303.11873.

<a class='citeend' key='DoubleDescent'></a>[Unifying Grokking and Double Descent](https://arxiv.org/pdf/2303.06173.pdf)
Davies, X., Langosco, L., & Krueger, D. (2022, November). In NeurIPS ML Safety Workshop.

<a class='citeend' key='Multiscale'></a> [Multi-Scale Feature Learning Dynamics: Insights for Double Descent](https://proceedings.mlr.press/v162/pezeshki22a/pezeshki22a.pdf)
Pezeshki, M., Mitra, A., Bengio, Y., & Lajoie, G. (2022, June). In the International Conference on Machine Learning (pp. 17669-17690). PMLR.

<a class='citeend' key='EffectiveTheory'></a>[Towards Understanding Grokking: An Effective Theory of Representation Learning](https://arxiv.org/pdf/2205.10343.pdf)
Liu, Z., Kitouni, O., Nolte, N. S., Michaud, E., Tegmark, M., & Williams, M. (2022). Advances in Neural Information Processing Systems, 35, 34651-34663.

<a class='citeend' key='TMOS'></a>[Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)
Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., Grosse, R., McCandlish, S., Kaplan, J., Amodei, D., Wattenberg, M. and Olah, C., 2022. Transformer Circuits Thread.

<a class='citeend' key='superposition'></a>[Superposition, Memorization, and Double Descent](https://transformer-circuits.pub/2023/toy-double-descent/index.html)
Henighan, T., Carter, S., Hume, T., Elhage, N., Lasenby, R., Fort, S., Schiefer, N., and Olah, C., 2023. Transformer Circuits Thread.

<a class='citeend' key='Goldilocks'></a>[The Goldilocks Zone: Towards Better Understanding of Neural Network Loss Landscapes](https://arxiv.org/pdf/1807.02581.pdf)
Fort, S., & Scherlis, A. (2019, July). In Proceedings of the AAAI conference on artificial intelligence (Vol. 33, No. 01, pp. 3574-3581).

<a class='citeend' key='StructuralGrokking'></a> [Grokking of Hierarchical Structure in Vanilla Transformers](https://arxiv.org/pdf/2305.18741.pdf)
Murty, S., Sharma, P., Andreas, J., & Manning, C. D. (2023). arXiv preprint arXiv:2305.18741.

<a class='citeend' key='PredictingGrokking'></a> [Predicting Grokking Long Before it Happens: A Look Into the Loss Landscape of Models Which Grok](https://arxiv.org/pdf/2306.13253.pdf)
Notsawo Jr, P., Zhou, H., Pezeshki, M., Rish, I., & Dumas, G. (2023). arXiv preprint arXiv:2306.13253.

<a class='citeend' key='explain'></a>[Language models can explain neurons in language models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)
Bills, S., Cammarata, N., Mossing, D., Tillman, H., Gao, L., Goh, G., Sutskever, I., Leike, J., Wu, J., & Saunders, W. 2023. OpenAI Blog

<a class='citeend' key='Slingshot'></a> [The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon](https://arxiv.org/pdf/2206.04817.pdf)
Thilak, V., Littwin, E., Zhai, S., Saremi, O., Paiss, R., & Susskind, J. (2022). arXiv preprint arXiv:2206.04817.

<a class='citeend' key='double-demystified'></a>[Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle](https://arxiv.org/abs/2303.14151) Rylan Schaeffer, R., Khona, M., Robertson, Z., Boopathy, A., Pistunova, K., Rocks, J., Rani Fiete, I., & Koyejo, O. (2023). arXiv preprint arXiv:2303.14151.

<a class='citeend' key='quantization'></a>[The Quantization Model of Neural Scaling](https://arxiv.org/abs/2303.13506) Eric J. Michaud, Ziming Liu, Uzay Girit, Max Tegmark, O. (2023). arXiv preprint arXiv:2303.13506.

<a class='citeend' key='multiple-choice'></a>[Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla](https://arxiv.org/abs/2307.09458) Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, Vladimir Mikulik (2023). arXiv preprint arXiv:2307.09458.

<a class='citeend' key='Connectome'></a> [The Connectome of an Insect Brain](https://www.science.org/doi/abs/10.1126/science.add9330) 
Winding, M., Pedigo, B. D., Barnes, C. L., Patsolic, H. G., Park, Y., Kazimiers, T., ... & Zlatic, M. (2023). Science, 379(6636), eadd9330.


### More Explorables

<p id='recirc'></p>
<div class='recirc-feedback-form'></div>


<link rel='stylesheet' href='../third_party/footnote_v2.css'>
<link rel='stylesheet' href='../third_party/citation_v2.css'>
<link rel='stylesheet' href='style.css'>

<script id='MathJax-script' async src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/mathtex-script-type.min.js" integrity="sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT" crossorigin="anonymous"></script>

<script src='../third_party/d3_.js'></script>
<script src='../third_party/d3-scale-chromatic.v1.min.js'></script>
<script src='../../third_party/tfjsv3.18.0.js'></script>
<script src='../../third_party/npyjs-global.js'></script>
<script src='../third_party/swoopy-drag.js'></script>

<script src='../third_party/footnote_v2.js'></script>
<script src='../third_party/citation_v2.js'></script>

<script src='util.js'></script>
<script src='init-accuracy-chart.js'></script>
<script src='init-animate-steps.js'></script>
<script src='init-embed-vis.js'></script>
<script src='init-input-sliders.js'></script>
<script src='init-swoopy.js'></script>

<link rel='stylesheet' href='mod-top/style.css'>
<script src='mod-top/init.js'></script>
<script src='mod-top/init-waves.js'></script>

<link rel='stylesheet' href='sparse-parity/style.css'>
<script src='sparse-parity/init.js'></script>
<script src='sparse-parity/init-weight-trajectory.js'></script>

<link rel='stylesheet' href='sparse-parity-sweep/style.css'>
<script src='sparse-parity-sweep/init.js'></script>

<link rel='stylesheet' href='hand-weights/style.css'>
<link rel='stylesheet' href='hand-weights/sliders.css'>
<script src='hand-weights/init-embed-vis.js'></script>
<script src='hand-weights/init-activation-vis.js'></script>
<script src='hand-weights/init-circle-weights-vis.js'></script>
<script src='hand-weights/init-circle-input-vis.js'></script>
<script src='hand-weights/init-circle-weights-freq.js'></script>

<script src='hand-weights/init.js'></script>

<link rel='stylesheet' href='mod-bot/style.css'>
<script src='mod-bot/init.js'></script>
<script src='mod-bot/init-bot-freqs.js'></script>
<script src='mod-bot/init-bot-logits.js'></script>
<script src='mod-bot/seeds/init-seeds.js'></script>

<script src='open-q-mem/init-0.js'></script>
<script src='open-q-mem/init-1.js'></script>

<script src='circle-freq/circle-freq-init.js'></script>

<link rel='stylesheet' href='appendix/style.css'>
<script src='appendix/debug-vis.js'></script>
<script src='appendix/init-circle-weights-vis.js'></script>
<script src='appendix/init-proj-vis.js'></script>
<script src='appendix/init-sliders.js'></script>
<script src='appendix/line-error-vis.js'></script>
<script src='appendix/debug-reuleaux.js'></script>

<script src='appendix/init.js'></script>




<script src='../third_party/recirc.js'></script>